"""
Document Processor / æ–‡æ¡£å¤„ç†å™¨
================================

åŸºäº LlamaIndex çš„å¤šæ ¼å¼æ–‡æ¡£åŠ è½½å’Œåˆ†å—å¤„ç†æ¨¡å—ã€‚

æ ¸å¿ƒåŠŸèƒ½
--------
1. **å¤šæ ¼å¼æ”¯æŒ**: PDF, Word, Excel, Text, Markdown, HTML
2. **æ™ºèƒ½åˆ†å—**: é€’å½’å­—ç¬¦åˆ†å‰² / è¯­ä¹‰åˆ†å—
3. **å…ƒæ•°æ®æå–**: è‡ªåŠ¨æå–æ–‡ä»¶ä¿¡æ¯
4. **åŒæ ¼å¼å…¼å®¹**: åŒæ—¶æ”¯æŒ LlamaIndex å’Œ LangChain Document
5. **Langfuse è¿½è¸ª**: è‡ªåŠ¨è®°å½•æ–‡æ¡£å¤„ç†è¿‡ç¨‹

åˆ†å—ç­–ç•¥
--------
1. **é€’å½’å­—ç¬¦åˆ†å‰² (é»˜è®¤)**
   - æŒ‰å›ºå®šå¤§å°åˆ†å‰²
   - æ”¯æŒè‡ªå®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§
   - é€‚åˆé€šç”¨æ–‡æ¡£

2. **è¯­ä¹‰åˆ†å— (å¯é€‰)**
   - åŸºäºè¯­ä¹‰è¾¹ç•Œåˆ†å‰²
   - ä¿æŒæ®µè½å®Œæ•´æ€§
   - é€‚åˆé•¿æ–‡æ¡£å’ŒæŠ€æœ¯æ–‡æ¡£

Langfuse è¿½è¸ª
-------------
```
Trace: document_processing
â”œâ”€â”€ Span: load_file / load_directory
â””â”€â”€ Span: split_documents
    â”œâ”€â”€ chunk_count
    â”œâ”€â”€ avg_chunk_size
    â””â”€â”€ processing_time
```

é…ç½®å‚æ•°
--------
```python
# config/settings.py
RAG_CHUNK_SIZE = 500              # åˆ†å—å¤§å°
RAG_CHUNK_OVERLAP = 50            # é‡å å¤§å°
RAG_USE_SEMANTIC_CHUNKING = True  # ä½¿ç”¨è¯­ä¹‰åˆ†å—
LANGFUSE_ENABLED = True           # å¯ç”¨è¿½è¸ª
```

ä½¿ç”¨ç¤ºä¾‹
--------
```python
from app.core.document_processor import DocumentProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = DocumentProcessor(
    chunk_size=500,
    chunk_overlap=50,
    use_semantic_chunking=True
)

# å¤„ç†å•ä¸ªæ–‡ä»¶
docs = processor.process_file("/path/to/document.pdf")

# å¤„ç†ç›®å½•
docs = processor.process_directory("/path/to/knowledge_base/")

# å¤„ç†æ–‡æœ¬
docs = processor.process_text("è¿™æ˜¯ä¸€æ®µæ–‡æœ¬å†…å®¹...")

# è·å– LangChain æ ¼å¼ (ç”¨äºå‘é‡å­˜å‚¨)
langchain_docs = processor.to_langchain_documents(docs)
```

Author: Intelligent Customer Service Team
Version: 2.1.0 (LlamaIndex + Langfuse)
"""
import logging
import time
from typing import List, Optional, Dict, Any, Union
from pathlib import Path

from llama_index.core import Document as LlamaDocument
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
)
from llama_index.core.schema import TextNode, NodeWithScore

# LangChain Document for compatibility
from langchain_core.documents import Document as LangchainDocument

from config.settings import settings

# Langfuse observability
from app.services.langfuse_service import get_langfuse_service

logger = logging.getLogger(__name__)

# Try to import optional readers
PDF_READER_AVAILABLE = False
DOCX_READER_AVAILABLE = False
MARKDOWN_READER_AVAILABLE = False
CSV_READER_AVAILABLE = False
HTML_READER_AVAILABLE = False
EXCEL_READER_AVAILABLE = False

try:
    from llama_index.readers.file import PDFReader
    PDF_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] PDFReader not available, using default")

try:
    from llama_index.readers.file import DocxReader
    DOCX_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] DocxReader not available, using default")

try:
    from llama_index.readers.file import MarkdownReader
    MARKDOWN_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] MarkdownReader not available, using default")

try:
    from llama_index.readers.file import CSVReader
    CSV_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] CSVReader not available, using default")

try:
    from llama_index.readers.file import HTMLTagReader
    HTML_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] HTMLTagReader not available, using default")

try:
    from llama_index.readers.file import PandasExcelReader
    EXCEL_READER_AVAILABLE = True
except ImportError:
    logger.debug("[DocumentProcessor] PandasExcelReader not available, using default")


def _build_file_extractors() -> dict:
    """
    æ„å»ºæ–‡ä»¶æ‰©å±•ååˆ° Reader çš„æ˜ å°„ã€‚

    Returns
    -------
    dict
        æ–‡ä»¶æ‰©å±•ååˆ° Reader å®ä¾‹çš„æ˜ å°„
    """
    extractors = {}

    if DOCX_READER_AVAILABLE:
        extractors[".docx"] = DocxReader()
        extractors[".doc"] = DocxReader()

    if PDF_READER_AVAILABLE:
        extractors[".pdf"] = PDFReader()

    if MARKDOWN_READER_AVAILABLE:
        extractors[".md"] = MarkdownReader()

    if CSV_READER_AVAILABLE:
        extractors[".csv"] = CSVReader()

    if HTML_READER_AVAILABLE:
        extractors[".html"] = HTMLTagReader()
        extractors[".htm"] = HTMLTagReader()

    if EXCEL_READER_AVAILABLE:
        extractors[".xlsx"] = PandasExcelReader()
        extractors[".xls"] = PandasExcelReader()

    logger.info(f"[DocumentProcessor] File extractors configured: {list(extractors.keys())}")
    return extractors


# Global file extractors instance
FILE_EXTRACTORS = _build_file_extractors()


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ (LlamaIndex å®ç°)
    ===========================

    æä¾›å¤šæ ¼å¼æ–‡æ¡£çš„åŠ è½½ã€åˆ†å—å’Œè½¬æ¢åŠŸèƒ½ã€‚

    Attributes
    ----------
    chunk_size : int
        åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤ 500

    chunk_overlap : int
        åˆ†å—é‡å å¤§å°ï¼Œé»˜è®¤ 50

    use_semantic_chunking : bool
        æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼Œé»˜è®¤ False

    SUPPORTED_EXTENSIONS : dict
        æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åæ˜ å°„

    Example
    -------
    ```python
    processor = DocumentProcessor()

    # åŠ è½½ PDF
    docs = processor.load_file("manual.pdf")

    # åˆ†å—
    chunks = processor.split_documents(docs)

    # è½¬æ¢ä¸º LangChain æ ¼å¼
    lc_docs = processor.to_langchain_documents(chunks)
    ```
    """

    SUPPORTED_EXTENSIONS = {
        ".txt": "text",
        ".md": "markdown",
        ".pdf": "pdf",
        ".d"
        "ocx": "word",
        ".doc": "word",
        ".xlsx": "excel",
        ".xls": "excel",
        ".html": "html",
        ".htm": "html",
        ".csv": "csv",
        ".json": "json",
    }

    def __init__(
        self,
        chunk_size: int = None,
        chunk_overlap: int = None,
        use_semantic_chunking: bool = None,
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ã€‚

        Parameters
        ----------
        chunk_size : int, optional
            åˆ†å—å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨ settings.RAG_CHUNK_SIZE

        chunk_overlap : int, optional
            åˆ†å—é‡å ï¼Œé»˜è®¤ä½¿ç”¨ settings.RAG_CHUNK_OVERLAP

        use_semantic_chunking : bool, optional
            æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼Œé»˜è®¤ä½¿ç”¨ settings.RAG_USE_SEMANTIC_CHUNKING
        """
        self.chunk_size = chunk_size or settings.RAG_CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or settings.RAG_CHUNK_OVERLAP
        self.use_semantic_chunking = (
            use_semantic_chunking
            if use_semantic_chunking is not None
            else settings.RAG_USE_SEMANTIC_CHUNKING
        )

        # åˆå§‹åŒ–åˆ†å—å™¨
        self._init_splitters()

        logger.info(
            f"[DocumentProcessor] Initialized - chunk_size: {self.chunk_size}, "
            f"overlap: {self.chunk_overlap}, semantic: {self.use_semantic_chunking}"
        )

    def _init_splitters(self):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨ã€‚"""
        # å¥å­åˆ†å—å™¨ (é»˜è®¤)
        self.sentence_splitter = SentenceSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            # ä¸­è‹±æ–‡åˆ†éš”ç¬¦
            paragraph_separator="\n\n",
            secondary_chunking_regex="[ã€‚ï¼ï¼Ÿ.!?]",
        )

        # è¯­ä¹‰åˆ†å—å™¨ (å¯é€‰ï¼Œéœ€è¦åµŒå…¥æ¨¡å‹)
        self._semantic_splitter = None

    def _get_semantic_splitter(self):
        """æ‡’åŠ è½½è¯­ä¹‰åˆ†å—å™¨ã€‚"""
        if self._semantic_splitter is None:
            try:
                from llama_index.core import Settings as LlamaSettings

                if LlamaSettings.embed_model is not None:
                    self._semantic_splitter = SemanticSplitterNodeParser(
                        embed_model=LlamaSettings.embed_model,
                        buffer_size=settings.RAG_SEMANTIC_CHUNK_BUFFER_SIZE,
                        breakpoint_percentile_threshold=95,
                    )
                    logger.info("[DocumentProcessor] Semantic splitter initialized")
                else:
                    logger.warning(
                        "[DocumentProcessor] No embed model, falling back to sentence splitter"
                    )
            except Exception as e:
                logger.warning(f"[DocumentProcessor] Semantic splitter init failed: {e}")

        return self._semantic_splitter

    def load_file(
        self,
        file_path: str,
        trace=None,
    ) -> List[LlamaDocument]:
        """
        åŠ è½½å•ä¸ªæ–‡ä»¶ã€‚

        Parameters
        ----------
        file_path : str
            æ–‡ä»¶è·¯å¾„

        trace : Langfuse Trace, optional
            Langfuse è¿½è¸ªå¯¹è±¡

        Returns
        -------
        List[LlamaDocument]
            LlamaIndex Document åˆ—è¡¨

        Raises
        ------
        FileNotFoundError
            æ–‡ä»¶ä¸å­˜åœ¨

        ValueError
            ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹

        Example
        -------
        ```python
        docs = processor.load_file("/path/to/file.pdf")
        print(f"Loaded {len(docs)} documents")
        print(docs[0].text[:100])
        ```
        """
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        ext = path.suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            raise ValueError(f"Unsupported file type: {ext}")

        file_type = self.SUPPORTED_EXTENSIONS[ext]
        logger.info(f"[DocumentProcessor] Loading {file_type} file: {path.name}")

        # Langfuse è¿½è¸ª
        langfuse = get_langfuse_service()
        span = None
        if trace:
            span = langfuse.create_span(
                trace,
                name="load_file",
                input={
                    "file_path": path.name,
                    "file_type": file_type,
                    "file_size": path.stat().st_size,
                },
            )

        start_time = time.time()

        try:
            # ä½¿ç”¨ SimpleDirectoryReader åŠ è½½å•ä¸ªæ–‡ä»¶
            reader = SimpleDirectoryReader(
                input_files=[str(path)],
                filename_as_id=True,
                file_extractor=FILE_EXTRACTORS,  # ä½¿ç”¨æ˜¾å¼é…ç½®çš„æ–‡ä»¶è§£æå™¨
            )
            documents = reader.load_data()

            # æ·»åŠ å…ƒæ•°æ®
            for doc in documents:
                doc.metadata["source"] = path.name
                doc.metadata["file_type"] = file_type
                doc.metadata["file_path"] = str(path.absolute())

            elapsed = time.time() - start_time
            logger.info(f"[DocumentProcessor] Loaded {len(documents)} document(s)")

            # ç»“æŸ Span
            if span:
                langfuse.end_span(
                    span,
                    output={
                        "num_documents": len(documents),
                        "total_chars": sum(len(doc.text) for doc in documents),
                        "elapsed_seconds": round(elapsed, 3),
                    },
                )

            return documents

        except Exception as e:
            logger.error(f"[DocumentProcessor] Failed to load {file_path}: {e}")
            if span:
                langfuse.end_span(
                    span,
                    output={"error": str(e)},
                    level="ERROR",
                    status_message=str(e),
                )
            raise

    def load_directory(
        self,
        directory_path: str,
        recursive: bool = True,
        exclude_hidden: bool = True,
    ) -> List[LlamaDocument]:
        """
        åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ã€‚

        Parameters
        ----------
        directory_path : str
            ç›®å½•è·¯å¾„

        recursive : bool
            æ˜¯å¦é€’å½’å­ç›®å½•ï¼Œé»˜è®¤ True

        exclude_hidden : bool
            æ˜¯å¦æ’é™¤éšè—æ–‡ä»¶ï¼Œé»˜è®¤ True

        Returns
        -------
        List[LlamaDocument]
            æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨

        Example
        -------
        ```python
        docs = processor.load_directory(
            "/path/to/knowledge_base",
            recursive=True
        )
        ```
        """
        path = Path(directory_path)
        if not path.exists() or not path.is_dir():
            raise ValueError(f"Invalid directory: {directory_path}")

        logger.info(f"[DocumentProcessor] Loading directory: {directory_path}")

        try:
            # æ„å»ºæ”¯æŒçš„æ‰©å±•ååˆ—è¡¨
            required_exts = list(self.SUPPORTED_EXTENSIONS.keys())

            reader = SimpleDirectoryReader(
                input_dir=str(path),
                recursive=recursive,
                exclude_hidden=exclude_hidden,
                required_exts=required_exts,
                filename_as_id=True,
                file_extractor=FILE_EXTRACTORS,  # ä½¿ç”¨æ˜¾å¼é…ç½®çš„æ–‡ä»¶è§£æå™¨
            )
            documents = reader.load_data()

            # æ·»åŠ å…ƒæ•°æ®å¹¶è¾“å‡ºå¤„ç†çš„æ–‡æ¡£ä¿¡æ¯
            logger.info(f"[DocumentProcessor] ========== æ–‡æ¡£åŠ è½½è¯¦æƒ… ==========")
            for i, doc in enumerate(documents, 1):
                file_path = doc.metadata.get("file_path", "")
                if file_path:
                    p = Path(file_path)
                    doc.metadata["source"] = p.name
                    doc.metadata["file_type"] = self.SUPPORTED_EXTENSIONS.get(
                        p.suffix.lower(), "unknown"
                    )
                    file_size = p.stat().st_size if p.exists() else 0
                    logger.info(
                        f"[DocumentProcessor] ğŸ“„ æ–‡æ¡£ {i}/{len(documents)}: {p.name} "
                        f"| ç±»å‹: {doc.metadata['file_type']} "
                        f"| å¤§å°: {file_size/1024:.1f} KB "
                        f"| å†…å®¹é•¿åº¦: {len(doc.text)} å­—ç¬¦"
                    )

            logger.info(f"[DocumentProcessor] ========== åŠ è½½å®Œæˆ ==========")
            logger.info(
                f"[DocumentProcessor] âœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£"
            )
            return documents

        except Exception as e:
            logger.error(f"[DocumentProcessor] Failed to load directory: {e}")
            raise

    def split_documents(
        self,
        documents: List[LlamaDocument],
        use_semantic: bool = None,
        trace=None,
    ) -> List[TextNode]:
        """
        å°†æ–‡æ¡£åˆ†å‰²ä¸ºæ–‡æœ¬èŠ‚ç‚¹ã€‚

        Parameters
        ----------
        documents : List[LlamaDocument]
            å¾…åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨

        use_semantic : bool, optional
            æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼Œé»˜è®¤ä½¿ç”¨å®ä¾‹é…ç½®

        trace : Langfuse Trace, optional
            Langfuse è¿½è¸ªå¯¹è±¡

        Returns
        -------
        List[TextNode]
            åˆ†å‰²åçš„æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨

        Note
        ----
        è¯­ä¹‰åˆ†å—éœ€è¦åµŒå…¥æ¨¡å‹æ”¯æŒï¼Œå¦‚æœä¸å¯ç”¨ä¼šè‡ªåŠ¨é™çº§åˆ°å¥å­åˆ†å—ã€‚

        Example
        -------
        ```python
        docs = processor.load_file("document.pdf")
        nodes = processor.split_documents(docs)
        print(f"Split into {len(nodes)} chunks")
        ```
        """
        if not documents:
            return []

        use_semantic = use_semantic if use_semantic is not None else self.use_semantic_chunking

        logger.info(
            f"[DocumentProcessor] Splitting {len(documents)} documents "
            f"(semantic={use_semantic})"
        )

        # Langfuse è¿½è¸ª
        langfuse = get_langfuse_service()
        span = None
        if trace:
            span = langfuse.create_span(
                trace,
                name="split_documents",
                input={
                    "num_documents": len(documents),
                    "total_chars": sum(len(doc.text) for doc in documents),
                    "use_semantic": use_semantic,
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap,
                },
            )

        start_time = time.time()

        try:
            if use_semantic:
                splitter = self._get_semantic_splitter()
                if splitter is None:
                    splitter = self.sentence_splitter
                    logger.info("[DocumentProcessor] âš ï¸ è¯­ä¹‰åˆ†å—å™¨ä¸å¯ç”¨ï¼Œå›é€€åˆ°å¥å­åˆ†å—å™¨")
            else:
                splitter = self.sentence_splitter

            splitter_type = "è¯­ä¹‰åˆ†å—" if use_semantic and splitter != self.sentence_splitter else "å¥å­åˆ†å—"
            logger.info(f"[DocumentProcessor] ========== å¼€å§‹æ–‡æ¡£åˆ†å— ==========")
            logger.info(f"[DocumentProcessor] ğŸ”§ åˆ†å—ç­–ç•¥: {splitter_type}")
            logger.info(f"[DocumentProcessor] ğŸ“ åˆ†å—å¤§å°: {self.chunk_size} å­—ç¬¦, é‡å : {self.chunk_overlap} å­—ç¬¦")

            # åˆ†å‰²æ–‡æ¡£
            nodes = splitter.get_nodes_from_documents(documents)

            elapsed = time.time() - start_time
            avg_size = sum(len(n.text) for n in nodes) // max(len(nodes), 1)
            min_size = min(len(n.text) for n in nodes) if nodes else 0
            max_size = max(len(n.text) for n in nodes) if nodes else 0

            # è¾“å‡ºæ¯ä¸ªæ–‡æ¡£çš„åˆ†å—è¯¦æƒ…
            logger.info(f"[DocumentProcessor] ========== åˆ†å—è¯¦æƒ… ==========")
            doc_chunks = {}
            for node in nodes:
                source = node.metadata.get("source", "unknown")
                if source not in doc_chunks:
                    doc_chunks[source] = []
                doc_chunks[source].append(len(node.text))

            for source, chunk_sizes in doc_chunks.items():
                logger.info(
                    f"[DocumentProcessor] ğŸ“„ {source}: "
                    f"{len(chunk_sizes)} ä¸ªåˆ†å— | "
                    f"å¹³å‡: {sum(chunk_sizes)//len(chunk_sizes)} å­—ç¬¦ | "
                    f"èŒƒå›´: {min(chunk_sizes)}-{max(chunk_sizes)} å­—ç¬¦"
                )

            logger.info(f"[DocumentProcessor] ========== åˆ†å—å®Œæˆ ==========")
            logger.info(
                f"[DocumentProcessor] âœ… å…±ç”Ÿæˆ {len(nodes)} ä¸ªåˆ†å— | "
                f"å¹³å‡: {avg_size} å­—ç¬¦ | èŒƒå›´: {min_size}-{max_size} å­—ç¬¦ | "
                f"è€—æ—¶: {elapsed:.2f} ç§’"
            )

            # ç»“æŸ Span
            if span:
                langfuse.end_span(
                    span,
                    output={
                        "num_chunks": len(nodes),
                        "avg_chunk_size": avg_size,
                        "min_chunk_size": min(len(n.text) for n in nodes) if nodes else 0,
                        "max_chunk_size": max(len(n.text) for n in nodes) if nodes else 0,
                        "elapsed_seconds": round(elapsed, 3),
                        "splitter_type": "semantic" if use_semantic else "sentence",
                    },
                )

            return nodes

        except Exception as e:
            logger.error(f"[DocumentProcessor] Split failed: {e}")
            if span:
                langfuse.end_span(
                    span,
                    output={"error": str(e)},
                    level="ERROR",
                    status_message=str(e),
                )
            # é™çº§å¤„ç†ï¼šç›´æ¥è¿”å›æ–‡æ¡£å†…å®¹ä½œä¸ºå•ä¸ªèŠ‚ç‚¹
            return [
                TextNode(text=doc.text, metadata=doc.metadata)
                for doc in documents
            ]

    def process_file(
        self,
        file_path: str,
        split: bool = True,
        enable_trace: bool = True,
    ) -> Union[List[LlamaDocument], List[TextNode]]:
        """
        åŠ è½½å¹¶å¤„ç†å•ä¸ªæ–‡ä»¶ã€‚

        Parameters
        ----------
        file_path : str
            æ–‡ä»¶è·¯å¾„

        split : bool
            æ˜¯å¦åˆ†å—ï¼Œé»˜è®¤ True

        enable_trace : bool
            æ˜¯å¦å¯ç”¨ Langfuse è¿½è¸ªï¼Œé»˜è®¤ True

        Returns
        -------
        Union[List[LlamaDocument], List[TextNode]]
            å¦‚æœ split=True è¿”å› TextNode åˆ—è¡¨ï¼Œå¦åˆ™è¿”å› Document åˆ—è¡¨

        Example
        -------
        ```python
        # åŠ è½½å¹¶åˆ†å—
        nodes = processor.process_file("document.pdf", split=True)

        # ä»…åŠ è½½ä¸åˆ†å—
        docs = processor.process_file("document.pdf", split=False)
        ```
        """
        # åˆ›å»º Langfuse è¿½è¸ª
        langfuse = get_langfuse_service()
        trace = None

        if enable_trace and langfuse.enabled:
            path = Path(file_path)
            trace = langfuse.create_trace(
                name="document_processing",
                input={
                    "file_path": path.name,
                    "split": split,
                },
                metadata={
                    "file_type": path.suffix.lower(),
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap,
                    "use_semantic_chunking": self.use_semantic_chunking,
                },
                tags=["document_processing", path.suffix.lower().lstrip(".")],
            )

        try:
            documents = self.load_file(file_path, trace=trace)

            if split:
                nodes = self.split_documents(documents, trace=trace)

                # ç»“æŸè¿½è¸ª
                if trace:
                    langfuse.end_trace(
                        trace,
                        output={
                            "status": "success",
                            "num_documents": len(documents),
                            "num_chunks": len(nodes),
                        },
                    )
                return nodes

            if trace:
                langfuse.end_trace(
                    trace,
                    output={
                        "status": "success",
                        "num_documents": len(documents),
                    },
                )
            return documents

        except Exception as e:
            if trace:
                langfuse.end_trace(
                    trace,
                    output={"status": "error", "error": str(e)},
                    metadata={"error_type": type(e).__name__},
                )
            raise

    def process_directory(
        self,
        directory_path: str,
        split: bool = True,
        recursive: bool = True,
    ) -> Union[List[LlamaDocument], List[TextNode]]:
        """
        åŠ è½½å¹¶å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ã€‚

        Parameters
        ----------
        directory_path : str
            ç›®å½•è·¯å¾„

        split : bool
            æ˜¯å¦åˆ†å—ï¼Œé»˜è®¤ True

        recursive : bool
            æ˜¯å¦é€’å½’ï¼Œé»˜è®¤ True

        Returns
        -------
        Union[List[LlamaDocument], List[TextNode]]
            å¤„ç†åçš„æ–‡æ¡£æˆ–èŠ‚ç‚¹åˆ—è¡¨
        """
        documents = self.load_directory(directory_path, recursive=recursive)
        if split:
            return self.split_documents(documents)
        return documents

    def process_text(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
        split: bool = True,
    ) -> Union[List[LlamaDocument], List[TextNode]]:
        """
        å¤„ç†åŸå§‹æ–‡æœ¬ã€‚

        Parameters
        ----------
        text : str
            åŸå§‹æ–‡æœ¬å†…å®¹

        metadata : dict, optional
            å…ƒæ•°æ®å­—å…¸

        split : bool
            æ˜¯å¦åˆ†å—ï¼Œé»˜è®¤ True

        Returns
        -------
        Union[List[LlamaDocument], List[TextNode]]
            å¤„ç†åçš„æ–‡æ¡£æˆ–èŠ‚ç‚¹åˆ—è¡¨

        Example
        -------
        ```python
        nodes = processor.process_text(
            "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬...",
            metadata={"source": "user_input", "category": "FAQ"}
        )
        ```
        """
        doc = LlamaDocument(
            text=text,
            metadata=metadata or {"source": "direct_input"},
        )
        documents = [doc]

        if split:
            return self.split_documents(documents)
        return documents

    # ==================== æ ¼å¼è½¬æ¢ ====================

    def to_langchain_documents(
        self,
        nodes: Union[List[TextNode], List[LlamaDocument], List[NodeWithScore]],
    ) -> List[LangchainDocument]:
        """
        å°† LlamaIndex èŠ‚ç‚¹/æ–‡æ¡£è½¬æ¢ä¸º LangChain Documentã€‚

        ç”¨äºä¸ç°æœ‰çš„å‘é‡å­˜å‚¨å’Œå·¥å…·é“¾é›†æˆã€‚

        Parameters
        ----------
        nodes : List
            LlamaIndex TextNodeã€Document æˆ– NodeWithScore åˆ—è¡¨

        Returns
        -------
        List[LangchainDocument]
            LangChain Document åˆ—è¡¨

        Example
        -------
        ```python
        # LlamaIndex èŠ‚ç‚¹
        nodes = processor.process_file("doc.pdf")

        # è½¬æ¢ä¸º LangChain æ ¼å¼
        lc_docs = processor.to_langchain_documents(nodes)

        # ç”¨äºå‘é‡å­˜å‚¨
        vector_store.add_documents(lc_docs)
        ```
        """
        langchain_docs = []

        for item in nodes:
            # å¤„ç† NodeWithScore
            if isinstance(item, NodeWithScore):
                node = item.node
                text = node.get_content()
                metadata = dict(node.metadata) if node.metadata else {}
                metadata["score"] = item.score
            # å¤„ç† TextNode
            elif isinstance(item, TextNode):
                text = item.text
                metadata = dict(item.metadata) if item.metadata else {}
            # å¤„ç† LlamaDocument
            elif isinstance(item, LlamaDocument):
                text = item.text
                metadata = dict(item.metadata) if item.metadata else {}
            else:
                logger.warning(f"[DocumentProcessor] Unknown type: {type(item)}")
                continue

            langchain_docs.append(
                LangchainDocument(page_content=text, metadata=metadata)
            )

        return langchain_docs

    def from_langchain_documents(
        self,
        documents: List[LangchainDocument],
    ) -> List[LlamaDocument]:
        """
        å°† LangChain Document è½¬æ¢ä¸º LlamaIndex Documentã€‚

        Parameters
        ----------
        documents : List[LangchainDocument]
            LangChain Document åˆ—è¡¨

        Returns
        -------
        List[LlamaDocument]
            LlamaIndex Document åˆ—è¡¨
        """
        return [
            LlamaDocument(
                text=doc.page_content,
                metadata=dict(doc.metadata) if doc.metadata else {},
            )
            for doc in documents
        ]

    # ==================== å·¥å…·æ–¹æ³• ====================

    @staticmethod
    def get_supported_extensions() -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ã€‚"""
        return list(DocumentProcessor.SUPPORTED_EXTENSIONS.keys())

    @staticmethod
    def is_supported(file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒå¤„ç†ã€‚

        Parameters
        ----------
        file_path : str
            æ–‡ä»¶è·¯å¾„

        Returns
        -------
        bool
            æ˜¯å¦æ”¯æŒ
        """
        ext = Path(file_path).suffix.lower()
        return ext in DocumentProcessor.SUPPORTED_EXTENSIONS


# ==================== å…¨å±€å®ä¾‹ ====================

_document_processor: Optional[DocumentProcessor] = None


def get_document_processor() -> DocumentProcessor:
    """
    è·å–å…¨å±€æ–‡æ¡£å¤„ç†å™¨å®ä¾‹ã€‚

    Returns
    -------
    DocumentProcessor
        æ–‡æ¡£å¤„ç†å™¨å•ä¾‹

    Example
    -------
    ```python
    from app.core.document_processor import get_document_processor

    processor = get_document_processor()
    docs = processor.process_file("document.pdf")
    ```
    """
    global _document_processor
    if _document_processor is None:
        _document_processor = DocumentProcessor()
    return _document_processor
